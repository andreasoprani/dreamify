{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 149, 149, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 149, 149, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 147, 147, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 147, 147, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 147, 147, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 73, 73, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 73, 73, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 73, 73, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 71, 71, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 71, 71, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 71, 71, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 35, 35, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 35, 35, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 35, 35, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 35, 35, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 17, 17, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 17, 17, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 17, 17, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 17, 17, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 17, 17, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 8, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8, 8, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,851,784\n",
      "Trainable params: 23,817,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = inception_v3.InceptionV3(weights='imagenet', include_top=True)\n",
    "model._layers[0].batch_input_shape = (None, None, None, 3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\" Preprocess image for inception_v3 \"\"\"\n",
    "    \n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    \"\"\" Deprocess image from inception_v3 tensor \"\"\"\n",
    "    \n",
    "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    # Undo inception_v3 preprocess\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255\n",
    "\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image, feature_extractor, layer_settings):\n",
    "    \"\"\"\n",
    "    Computes loss based on a feature extractor and the corresponding layers weights.\n",
    "    \n",
    "    Parameters:\n",
    "        input_image (np.ndarray): numpy array of an image.\n",
    "        feature_extractor (tf.Model): feature extractor with outputs corresponding to the layers.\n",
    "        layer_settings (dict): dict with tf model layer names and corresponding weights in the loss function.\n",
    "        \n",
    "    Returns:\n",
    "        loss (float): loss of the image w.r.t. the layers specified in layer_settings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes features from image\n",
    "    features = feature_extractor(input_image)\n",
    "    \n",
    "    loss = tf.zeros(shape=())\n",
    "    for key in features.keys():\n",
    "        # Coefficient of the layer\n",
    "        coeff = layer_settings[key]\n",
    "        # Activation of the layer\n",
    "        activation = features[key]\n",
    "        # Adds to loss (avoid artifacts by removing borders)\n",
    "        scaling = tf.reduce_prod(tf.cast(tf.shape(activation), 'float32'))\n",
    "        #loss += coeff * tf.reduce_sum(tf.square(activation[:,2:-2,2:-2,:])) / scaling\n",
    "        loss += coeff * tf.reduce_sum(tf.square(activation)) / scaling\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def gradient_ascent_step(img, feature_extractor, layer_settings, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient ascent step on an image.\n",
    "    \n",
    "    Parameters:\n",
    "        img (np.ndarray): numpy array of an image.\n",
    "        feature_extractor (tf.Model): feature extractor with outputs corresponding to the layers.\n",
    "        layer_settings (dict): dict with tf model layer names and corresponding weights in the loss function.\n",
    "        learning_rate (float): learning rate for the gradient ascent step.\n",
    "        \n",
    "    Returns:\n",
    "        loss (float32): loss of the image w.r.t. the layers specified in layer_settings.\n",
    "        img (np.ndarray): numpy array of the modified image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes loss with GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, feature_extractor, layer_settings)\n",
    "    # Computes gradients and normalize\n",
    "    grads = tape.gradient(loss, img)\n",
    "    grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)\n",
    "    # Gradient ascent step\n",
    "    img += learning_rate * grads\n",
    "    \n",
    "    return loss, img\n",
    "\n",
    "def gradient_ascent_loop(img, feature_extractor, layer_settings, iterations, learning_rate, octave, shape, destination_filepath=None, max_loss=None):\n",
    "    \"\"\"\n",
    "    Performs the gradient ascent loop on an image.\n",
    "    \n",
    "    Parameters:\n",
    "        img (np.ndarray): numpy array of an image.\n",
    "        feature_extractor (tf.Model): feature extractor with outputs corresponding to the layers.\n",
    "        layer_settings (dict): dict with tf model layer names and corresponding weights in the loss function.\n",
    "        iterations (int): number of iterations for the loop.\n",
    "        learning_rate (float): learning rate for the gradient ascent steps.\n",
    "        max_loss (float): maximum loss before interruption (default=None).\n",
    "        \n",
    "    Returns:\n",
    "        img (np.ndarray): numpy array of the modified image\n",
    "    \"\"\"\n",
    "    \n",
    "    # gradient ascent loop\n",
    "    losses = []\n",
    "    t = tqdm(range(iterations))\n",
    "    for i in t:\n",
    "        loss, img = gradient_ascent_step(img, feature_extractor, layer_settings, learning_rate)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        description = \"\"\n",
    "        if destination_filepath:\n",
    "            description += destination_filepath + \" - \"\n",
    "        description += \"Octave: {0:d}, Shape: {1:s}, Loss: [{2:.2f}, {3:.2f}]\".format(octave, str(shape), np.min(losses), np.max(losses))\n",
    "        t.set_description(description)\n",
    "        t.refresh()\n",
    "        if max_loss is not None and loss > max_loss:\n",
    "            break\n",
    "        #print(\"Loss at step %d: %.2f\" % (i, loss))\n",
    "    #print(\"Min loss: %.2f - Max loss: %.2f\" % (np.min(losses), np.max(losses)))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dreamify(source_filepath,\n",
    "             destination_filepath,\n",
    "             model,\n",
    "             layer_settings,\n",
    "             learning_rate = 0.01,\n",
    "             num_octave = 3,\n",
    "             octave_scale = 1.5,\n",
    "             iterations = 20,\n",
    "             max_loss = None):\n",
    "    \"\"\"\n",
    "    Dreamifies an image.\n",
    "    Returns nothing, the image is automatically saved at the requested destination.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): path to the image folder (must end with \"/\").\n",
    "        source_filename (str): name of the original file (with file format).\n",
    "        destination_filename (str): name of the destination file (without file format).\n",
    "        model (tf.Model): model to be used.\n",
    "        layer_settings (dict): dict with tf model layer names and corresponding weights in the loss function.\n",
    "        learning_rate (float): learning rate for the gradient ascent steps (deault=0.01).\n",
    "        num_octave (int): number of subsampling octaves (default=3).\n",
    "        octave_scale (float): scale of each subsampling (deault=1.5).\n",
    "        iterations (int): number of iterations for the gradient ascent loop at each scale (deafault=20).\n",
    "        max_loss (float): maximum loss before interruption on a loop (default=15.0).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dict of output layers\n",
    "    outputs_dict = dict([(layer.name, layer.output)\n",
    "                         for layer in [model.get_layer(key)\n",
    "                                       for key in layer_settings.keys()]])\n",
    "    # Feature extractor from model input layer and dict of output layers\n",
    "    feature_extractor = keras.Model(inputs = model.inputs,\n",
    "                                    outputs = outputs_dict)\n",
    "    \n",
    "    # Loads image, preprocesses it and gets its shape\n",
    "    original_img = keras.preprocessing.image.load_img(source_filepath)\n",
    "    original_img = preprocess_image(original_img)\n",
    "    original_shape = original_img.shape[1:3]\n",
    "    \n",
    "    # Creates the list of successive shapes to use\n",
    "    successive_shapes = [original_shape]\n",
    "    for i in range(1, num_octave):\n",
    "        shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "        successive_shapes.append(shape)\n",
    "    successive_shapes = successive_shapes[::-1] # Invert\n",
    "    \n",
    "    # Image at minimum shape\n",
    "    shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n",
    "    # Image to be modified\n",
    "    img = tf.identity(original_img)\n",
    "    \n",
    "    for i, shape in enumerate(successive_shapes):\n",
    "        #print(\"Octave %d with shape %s\" % (i, shape))\n",
    "        # Resizes at current shape\n",
    "        img = tf.image.resize(img, shape)\n",
    "        # Performs the gradient ascent loop on the current shape\n",
    "        img = gradient_ascent_loop(img, \n",
    "                                   feature_extractor, \n",
    "                                   layer_settings,\n",
    "                                   iterations=iterations,\n",
    "                                   learning_rate=learning_rate,\n",
    "                                   octave=i,\n",
    "                                   shape=shape,\n",
    "                                   destination_filepath=destination_filepath,\n",
    "                                   max_loss=max_loss)\n",
    "        # Restores lost details\n",
    "        upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n",
    "        same_size_original = tf.image.resize(original_img, shape)\n",
    "        lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "        img += lost_detail\n",
    "        # Resizes the minimum shape to the current shape for next loop\n",
    "        shrunk_original_img = tf.image.resize(original_img, shape)\n",
    "        \n",
    "    # Stores the resulting image\n",
    "    keras.preprocessing.image.save_img(destination_filepath + '.png', deprocess_image(img.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_dreamify(source_filepath, layer_index, coeff, max_loss=None, display_image=False):\n",
    "    \"\"\" \n",
    "    Dreamifies an image using a single layer.\n",
    "    Returns nothing, the image is automatically saved at the requested destination.\n",
    "    \n",
    "    Parameters:\n",
    "        source_filepath (str) = filepath of the source file.\n",
    "        layer_index (int) = index of the layer to use.\n",
    "        coeff (float) = coefficient of the layer in the layer_settings dict.\n",
    "        max_loss (float) = maximum loss before breaking loop (default=None).\n",
    "        display_image (bool) = display the result (default=False).\n",
    "    \"\"\"\n",
    "    \n",
    "    layer_settings = {model.layers[layer_index].name : coeff}\n",
    "    \n",
    "    destination_filepath = source_filepath[:-4] + \"->\" + str(layer_index) + \"=\" + str(np.around(coeff, decimals=2))\n",
    "    \n",
    "    dreamify(source_filepath, destination_filepath, model, layer_settings, max_loss=max_loss)\n",
    "    \n",
    "    if display_image:\n",
    "        display(Image(destination_filepath + '.png'))\n",
    "        \n",
    "    return destination_filepath + '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dreams(file_path, depth, n_images, seed=0, max_coeff=10, max_loss=20):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for n in range(n_images):\n",
    "        \n",
    "        image_path = file_path\n",
    "        \n",
    "        for d in range(depth):\n",
    "\n",
    "            layer = np.random.randint(0, len(model.layers))\n",
    "            coeff = np.random.uniform(high=max_coeff)\n",
    "            \n",
    "            image_path = single_layer_dreamify(image_path, layer, coeff, max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images/Personal-Propic/o->172=5.93 - Octave: 0, Shape: (382, 382), Loss: [4.42, 21.35]:  40%|      | 8/20 [00:05<00:08,  1.38it/s]\n",
      "Images/Personal-Propic/o->172=5.93 - Octave: 1, Shape: (573, 573), Loss: [5.18, 20.37]:  25%|       | 5/20 [00:08<00:24,  1.66s/it]\n",
      "Images/Personal-Propic/o->172=5.93 - Octave: 2, Shape: (860, 860), Loss: [4.48, 20.41]:  25%|       | 5/20 [00:18<00:54,  3.62s/it]\n",
      "Images/Personal-Propic/o->172=5.93->192=6.03 - Octave: 0, Shape: (382, 382), Loss: [1.82, 20.52]:  70%|   | 14/20 [00:10<00:04,  1.39it/s]\n",
      "Images/Personal-Propic/o->172=5.93->192=6.03 - Octave: 1, Shape: (573, 573), Loss: [1.77, 21.11]:  65%|   | 13/20 [00:19<00:10,  1.52s/it]\n",
      "Images/Personal-Propic/o->172=5.93->192=6.03 - Octave: 2, Shape: (860, 860), Loss: [1.48, 21.51]:  70%|   | 14/20 [00:47<00:20,  3.39s/it]\n",
      "Images/Personal-Propic/o->172=5.93->192=6.03->195=8.47 - Octave: 0, Shape: (382, 382), Loss: [4.41, 20.18]:  20%|        | 4/20 [00:04<00:18,  1.16s/it]\n",
      "Images/Personal-Propic/o->172=5.93->192=6.03->195=8.47 - Octave: 1, Shape: (573, 573), Loss: [3.85, 22.06]:  30%|       | 6/20 [00:11<00:26,  1.86s/it]\n",
      "Images/Personal-Propic/o->172=5.93->192=6.03->195=8.47 - Octave: 2, Shape: (860, 860), Loss: [4.00, 21.61]:  35%|      | 7/20 [00:26<00:49,  3.80s/it]\n",
      "Images/Personal-Propic/o->211=6.46 - Octave: 0, Shape: (382, 382), Loss: [6.00, 21.78]:  30%|       | 6/20 [00:06<00:15,  1.10s/it]\n",
      "Images/Personal-Propic/o->211=6.46 - Octave: 1, Shape: (573, 573), Loss: [6.11, 21.16]:  20%|        | 4/20 [00:09<00:37,  2.37s/it]\n",
      "Images/Personal-Propic/o->211=6.46 - Octave: 2, Shape: (860, 860), Loss: [5.47, 21.21]:  20%|        | 4/20 [00:18<01:15,  4.74s/it]\n",
      "Images/Personal-Propic/o->211=6.46->292=2.98 - Octave: 0, Shape: (382, 382), Loss: [0.04, 2.57]: 100%|| 20/20 [00:18<00:00,  1.07it/s]\n",
      "Images/Personal-Propic/o->211=6.46->292=2.98 - Octave: 1, Shape: (573, 573), Loss: [0.06, 2.84]: 100%|| 20/20 [00:39<00:00,  1.96s/it]\n",
      "Images/Personal-Propic/o->211=6.46->292=2.98 - Octave: 2, Shape: (860, 860), Loss: [0.06, 3.18]: 100%|| 20/20 [01:24<00:00,  4.23s/it]\n",
      "Images/Personal-Propic/o->211=6.46->292=2.98->88=2.73 - Octave: 0, Shape: (382, 382), Loss: [3.34, 15.33]: 100%|| 20/20 [00:09<00:00,  2.08it/s]\n",
      "Images/Personal-Propic/o->211=6.46->292=2.98->88=2.73 - Octave: 1, Shape: (573, 573), Loss: [4.07, 18.16]: 100%|| 20/20 [00:20<00:00,  1.02s/it]\n",
      "Images/Personal-Propic/o->211=6.46->292=2.98->88=2.73 - Octave: 2, Shape: (860, 860), Loss: [4.38, 20.10]:  55%|    | 11/20 [00:27<00:22,  2.50s/it]\n",
      "Images/Personal-Propic/o->193=7.92 - Octave: 0, Shape: (382, 382), Loss: [1.00, 19.48]: 100%|| 20/20 [00:13<00:00,  1.50it/s]\n",
      "Images/Personal-Propic/o->193=7.92 - Octave: 1, Shape: (573, 573), Loss: [1.85, 21.44]:  60%|    | 12/20 [00:18<00:12,  1.57s/it]\n",
      "Images/Personal-Propic/o->193=7.92 - Octave: 2, Shape: (860, 860), Loss: [1.79, 20.07]:  40%|      | 8/20 [00:28<00:43,  3.62s/it]\n",
      "Images/Personal-Propic/o->193=7.92->87=4.8 - Octave: 0, Shape: (382, 382), Loss: [3.64, 10.62]: 100%|| 20/20 [00:09<00:00,  2.15it/s]\n",
      "Images/Personal-Propic/o->193=7.92->87=4.8 - Octave: 1, Shape: (573, 573), Loss: [4.21, 11.85]: 100%|| 20/20 [00:19<00:00,  1.00it/s]\n",
      "Images/Personal-Propic/o->193=7.92->87=4.8 - Octave: 2, Shape: (860, 860), Loss: [4.33, 20.86]:  90%| | 18/20 [00:42<00:04,  2.34s/it]\n",
      "Images/Personal-Propic/o->193=7.92->87=4.8->165=8.36 - Octave: 0, Shape: (382, 382), Loss: [6.15, 21.94]:  15%|        | 3/20 [00:03<00:21,  1.26s/it]\n",
      "Images/Personal-Propic/o->193=7.92->87=4.8->165=8.36 - Octave: 1, Shape: (573, 573), Loss: [5.79, 23.28]:  15%|        | 3/20 [00:06<00:37,  2.20s/it]\n",
      "Images/Personal-Propic/o->193=7.92->87=4.8->165=8.36 - Octave: 2, Shape: (860, 860), Loss: [5.23, 20.71]:  15%|        | 3/20 [00:13<01:15,  4.41s/it]\n",
      "Images/Personal-Propic/o->72=0.87 - Octave: 0, Shape: (382, 382), Loss: [0.28, 1.58]: 100%|| 20/20 [00:08<00:00,  2.40it/s]\n",
      "Images/Personal-Propic/o->72=0.87 - Octave: 1, Shape: (573, 573), Loss: [0.38, 1.88]: 100%|| 20/20 [00:17<00:00,  1.11it/s]\n",
      "Images/Personal-Propic/o->72=0.87 - Octave: 2, Shape: (860, 860), Loss: [0.38, 1.97]: 100%|| 20/20 [00:39<00:00,  1.98s/it]\n",
      "Images/Personal-Propic/o->72=0.87->115=3.68 - Octave: 0, Shape: (382, 382), Loss: [4.16, 21.73]:  25%|       | 5/20 [00:03<00:11,  1.32it/s]\n",
      "Images/Personal-Propic/o->72=0.87->115=3.68 - Octave: 1, Shape: (573, 573), Loss: [5.20, 21.05]:  20%|        | 4/20 [00:06<00:25,  1.57s/it]\n",
      "Images/Personal-Propic/o->72=0.87->115=3.68 - Octave: 2, Shape: (860, 860), Loss: [4.87, 22.92]:  25%|       | 5/20 [00:15<00:46,  3.09s/it]\n",
      "Images/Personal-Propic/o->72=0.87->115=3.68->197=7.78 - Octave: 0, Shape: (382, 382), Loss: [3.42, 20.67]:  65%|   | 13/20 [00:10<00:05,  1.28it/s]\n",
      "Images/Personal-Propic/o->72=0.87->115=3.68->197=7.78 - Octave: 1, Shape: (573, 573), Loss: [3.55, 20.75]:  65%|   | 13/20 [00:21<00:11,  1.63s/it]\n",
      "Images/Personal-Propic/o->72=0.87->115=3.68->197=7.78 - Octave: 2, Shape: (860, 860), Loss: [3.25, 20.01]:  70%|   | 14/20 [00:49<00:21,  3.53s/it]\n",
      "Images/Personal-Propic/o->99=7.99 - Octave: 0, Shape: (382, 382), Loss: [11.64, 21.72]:  15%|        | 3/20 [00:02<00:14,  1.14it/s]\n",
      "Images/Personal-Propic/o->99=7.99 - Octave: 1, Shape: (573, 573), Loss: [11.88, 21.98]:  15%|        | 3/20 [00:04<00:26,  1.55s/it]\n",
      "Images/Personal-Propic/o->99=7.99 - Octave: 2, Shape: (860, 860), Loss: [11.61, 21.39]:  15%|        | 3/20 [00:09<00:54,  3.20s/it]\n",
      "Images/Personal-Propic/o->99=7.99->243=5.2 - Octave: 0, Shape: (382, 382), Loss: [5.29, 31.09]:  10%|         | 2/20 [00:04<00:38,  2.12s/it]\n",
      "Images/Personal-Propic/o->99=7.99->243=5.2 - Octave: 1, Shape: (573, 573), Loss: [7.27, 24.37]:  10%|         | 2/20 [00:06<01:01,  3.44s/it]\n",
      "Images/Personal-Propic/o->99=7.99->243=5.2 - Octave: 2, Shape: (860, 860), Loss: [6.27, 24.28]:  10%|         | 2/20 [00:12<01:55,  6.42s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 20:26:34.079732 4814732736 def_function.py:474] 5 out of the last 14 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->99=7.99->243=5.2->147=1.18 - Octave: 0, Shape: (382, 382), Loss: [1.46, 17.59]: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n",
      "Images/Personal-Propic/o->99=7.99->243=5.2->147=1.18 - Octave: 1, Shape: (573, 573), Loss: [2.36, 19.38]: 100%|| 20/20 [00:24<00:00,  1.22s/it]\n",
      "Images/Personal-Propic/o->99=7.99->243=5.2->147=1.18 - Octave: 2, Shape: (860, 860), Loss: [2.09, 18.11]: 100%|| 20/20 [00:54<00:00,  2.71s/it]\n",
      "Images/Personal-Propic/o->288=5.82 - Octave: 0, Shape: (382, 382), Loss: [0.78, 16.19]: 100%|| 20/20 [00:17<00:00,  1.13it/s]\n",
      "Images/Personal-Propic/o->288=5.82 - Octave: 1, Shape: (573, 573), Loss: [1.02, 11.14]: 100%|| 20/20 [00:37<00:00,  1.87s/it]\n",
      "Images/Personal-Propic/o->288=5.82 - Octave: 2, Shape: (860, 860), Loss: [0.91, 8.72]: 100%|| 20/20 [01:21<00:00,  4.05s/it]\n",
      "Images/Personal-Propic/o->288=5.82->185=9.45 - Octave: 0, Shape: (382, 382), Loss: [3.37, 20.44]:  35%|      | 7/20 [00:06<00:11,  1.13it/s]\n",
      "Images/Personal-Propic/o->288=5.82->185=9.45 - Octave: 1, Shape: (573, 573), Loss: [4.14, 20.76]:  30%|       | 6/20 [00:10<00:24,  1.77s/it]\n",
      "Images/Personal-Propic/o->288=5.82->185=9.45 - Octave: 2, Shape: (860, 860), Loss: [3.44, 21.06]:  35%|      | 7/20 [00:25<00:46,  3.61s/it]\n",
      "Images/Personal-Propic/o->288=5.82->185=9.45->31=1.06 - Octave: 0, Shape: (382, 382), Loss: [2.44, 21.15]:  70%|   | 14/20 [00:04<00:01,  3.16it/s]\n",
      "Images/Personal-Propic/o->288=5.82->185=9.45->31=1.06 - Octave: 1, Shape: (573, 573), Loss: [3.41, 21.09]:  75%|  | 15/20 [00:10<00:03,  1.47it/s]\n",
      "Images/Personal-Propic/o->288=5.82->185=9.45->31=1.06 - Octave: 2, Shape: (860, 860), Loss: [3.13, 20.53]:  80%|  | 16/20 [00:23<00:05,  1.49s/it]\n",
      "Images/Personal-Propic/o->151=2.65 - Octave: 0, Shape: (382, 382), Loss: [0.46, 4.64]: 100%|| 20/20 [00:11<00:00,  1.77it/s]\n",
      "Images/Personal-Propic/o->151=2.65 - Octave: 1, Shape: (573, 573), Loss: [0.68, 6.34]: 100%|| 20/20 [00:24<00:00,  1.21s/it]\n",
      "Images/Personal-Propic/o->151=2.65 - Octave: 2, Shape: (860, 860), Loss: [0.70, 7.41]: 100%|| 20/20 [00:53<00:00,  2.67s/it]\n",
      "Images/Personal-Propic/o->151=2.65->183=2.17 - Octave: 0, Shape: (382, 382), Loss: [0.67, 4.68]: 100%|| 20/20 [00:12<00:00,  1.58it/s]\n",
      "Images/Personal-Propic/o->151=2.65->183=2.17 - Octave: 1, Shape: (573, 573), Loss: [0.66, 5.14]: 100%|| 20/20 [00:26<00:00,  1.34s/it]\n",
      "Images/Personal-Propic/o->151=2.65->183=2.17 - Octave: 2, Shape: (860, 860), Loss: [0.55, 5.04]: 100%|| 20/20 [00:59<00:00,  2.97s/it]\n",
      "Images/Personal-Propic/o->151=2.65->183=2.17->128=0.19 - Octave: 0, Shape: (382, 382), Loss: [0.25, 1.16]: 100%|| 20/20 [00:10<00:00,  1.93it/s]\n",
      "Images/Personal-Propic/o->151=2.65->183=2.17->128=0.19 - Octave: 1, Shape: (573, 573), Loss: [0.23, 1.18]: 100%|| 20/20 [00:22<00:00,  1.10s/it]\n",
      "Images/Personal-Propic/o->151=2.65->183=2.17->128=0.19 - Octave: 2, Shape: (860, 860), Loss: [0.19, 1.15]: 100%|| 20/20 [00:49<00:00,  2.46s/it]\n",
      "Images/Personal-Propic/o->53=1.5 - Octave: 0, Shape: (382, 382), Loss: [0.91, 7.90]: 100%|| 20/20 [00:07<00:00,  2.69it/s]\n",
      "Images/Personal-Propic/o->53=1.5 - Octave: 1, Shape: (573, 573), Loss: [1.68, 8.26]: 100%|| 20/20 [00:16<00:00,  1.24it/s]\n",
      "Images/Personal-Propic/o->53=1.5 - Octave: 2, Shape: (860, 860), Loss: [1.62, 8.00]: 100%|| 20/20 [00:35<00:00,  1.77s/it]\n",
      "Images/Personal-Propic/o->53=1.5->244=3.86 - Octave: 0, Shape: (382, 382), Loss: [2.85, 26.41]:  10%|         | 2/20 [00:04<00:39,  2.20s/it]\n",
      "Images/Personal-Propic/o->53=1.5->244=3.86 - Octave: 1, Shape: (573, 573), Loss: [2.83, 38.55]:  15%|        | 3/20 [00:08<00:47,  2.80s/it]\n",
      "Images/Personal-Propic/o->53=1.5->244=3.86 - Octave: 2, Shape: (860, 860), Loss: [2.75, 25.23]:  15%|        | 3/20 [00:16<01:33,  5.51s/it]\n",
      "Images/Personal-Propic/o->53=1.5->244=3.86->105=4.5 - Octave: 0, Shape: (382, 382), Loss: [8.90, 21.84]:  10%|         | 2/20 [00:02<00:21,  1.20s/it]\n",
      "Images/Personal-Propic/o->53=1.5->244=3.86->105=4.5 - Octave: 1, Shape: (573, 573), Loss: [6.74, 22.61]:  15%|        | 3/20 [00:05<00:29,  1.72s/it]\n",
      "Images/Personal-Propic/o->53=1.5->244=3.86->105=4.5 - Octave: 2, Shape: (860, 860), Loss: [6.26, 20.26]:  15%|        | 3/20 [00:10<01:00,  3.57s/it]\n",
      "Images/Personal-Propic/o->31=4.37 - Octave: 0, Shape: (382, 382), Loss: [9.94, 22.16]:  15%|        | 3/20 [00:01<00:07,  2.25it/s]\n",
      "Images/Personal-Propic/o->31=4.37 - Octave: 1, Shape: (573, 573), Loss: [11.77, 23.22]:  15%|        | 3/20 [00:02<00:15,  1.12it/s]\n",
      "Images/Personal-Propic/o->31=4.37 - Octave: 2, Shape: (860, 860), Loss: [11.02, 20.97]:  15%|        | 3/20 [00:05<00:33,  1.94s/it]\n",
      "Images/Personal-Propic/o->31=4.37->57=6.67 - Octave: 0, Shape: (382, 382), Loss: [8.67, 24.74]:  15%|        | 3/20 [00:01<00:10,  1.62it/s]\n",
      "Images/Personal-Propic/o->31=4.37->57=6.67 - Octave: 1, Shape: (573, 573), Loss: [11.11, 22.37]:  10%|         | 2/20 [00:02<00:25,  1.40s/it]\n",
      "Images/Personal-Propic/o->31=4.37->57=6.67 - Octave: 2, Shape: (860, 860), Loss: [10.17, 21.49]:  10%|         | 2/20 [00:05<00:51,  2.85s/it]\n",
      "Images/Personal-Propic/o->31=4.37->57=6.67->119=1.71 - Octave: 0, Shape: (382, 382), Loss: [1.26, 6.00]: 100%|| 20/20 [00:10<00:00,  1.95it/s]\n",
      "Images/Personal-Propic/o->31=4.37->57=6.67->119=1.71 - Octave: 1, Shape: (573, 573), Loss: [1.20, 6.08]: 100%|| 20/20 [00:21<00:00,  1.09s/it]\n",
      "Images/Personal-Propic/o->31=4.37->57=6.67->119=1.71 - Octave: 2, Shape: (860, 860), Loss: [1.13, 6.17]: 100%|| 20/20 [00:48<00:00,  2.44s/it]\n",
      "Images/Personal-Propic/o->82=1.29 - Octave: 0, Shape: (382, 382), Loss: [1.30, 6.13]: 100%|| 20/20 [00:08<00:00,  2.45it/s]\n",
      "Images/Personal-Propic/o->82=1.29 - Octave: 1, Shape: (573, 573), Loss: [1.76, 6.38]: 100%|| 20/20 [00:17<00:00,  1.15it/s]\n",
      "Images/Personal-Propic/o->82=1.29 - Octave: 2, Shape: (860, 860), Loss: [1.66, 6.33]: 100%|| 20/20 [00:38<00:00,  1.92s/it]\n",
      "Images/Personal-Propic/o->82=1.29->99=3.64 - Octave: 0, Shape: (382, 382), Loss: [9.67, 21.72]:  20%|        | 4/20 [00:02<00:11,  1.36it/s]\n",
      "Images/Personal-Propic/o->82=1.29->99=3.64 - Octave: 1, Shape: (573, 573), Loss: [9.46, 21.06]:  25%|       | 5/20 [00:06<00:19,  1.32s/it]\n",
      "Images/Personal-Propic/o->82=1.29->99=3.64 - Octave: 2, Shape: (860, 860), Loss: [8.98, 20.33]:  25%|       | 5/20 [00:14<00:42,  2.81s/it]\n",
      "Images/Personal-Propic/o->82=1.29->99=3.64->121=0.38 - Octave: 0, Shape: (382, 382), Loss: [0.40, 6.70]: 100%|| 20/20 [00:10<00:00,  1.87it/s]\n",
      "Images/Personal-Propic/o->82=1.29->99=3.64->121=0.38 - Octave: 1, Shape: (573, 573), Loss: [0.64, 8.05]: 100%|| 20/20 [00:22<00:00,  1.12s/it]\n",
      "Images/Personal-Propic/o->82=1.29->99=3.64->121=0.38 - Octave: 2, Shape: (860, 860), Loss: [0.57, 7.93]: 100%|| 20/20 [00:49<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "path = \"Images/Personal-Propic/o.jpg\"\n",
    "depth = 3\n",
    "n_images = 10\n",
    "\n",
    "generate_dreams(file_path = path, depth = depth, n_images = n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images/Personal-Propic/o->172=6.03 - Octave: 0, Shape: (382, 382), Loss: [4.49, 20.04]:  35%|      | 7/20 [00:06<00:12,  1.02it/s]\n",
      "Images/Personal-Propic/o->172=6.03 - Octave: 1, Shape: (573, 573), Loss: [5.18, 20.60]:  25%|       | 5/20 [00:07<00:22,  1.52s/it]\n",
      "Images/Personal-Propic/o->172=6.03 - Octave: 2, Shape: (860, 860), Loss: [4.65, 22.80]:  30%|       | 6/20 [00:18<00:43,  3.11s/it]\n",
      "Images/Personal-Propic/o->47=5.45 - Octave: 0, Shape: (382, 382), Loss: [9.63, 23.35]:  20%|        | 4/20 [00:01<00:07,  2.13it/s]\n",
      "Images/Personal-Propic/o->47=5.45 - Octave: 1, Shape: (573, 573), Loss: [11.35, 22.33]:  15%|        | 3/20 [00:03<00:19,  1.12s/it]\n",
      "Images/Personal-Propic/o->47=5.45 - Octave: 2, Shape: (860, 860), Loss: [12.06, 23.26]:  15%|        | 3/20 [00:06<00:38,  2.27s/it]\n",
      "Images/Personal-Propic/o->117=4.24 - Octave: 0, Shape: (382, 382), Loss: [1.53, 16.79]: 100%|| 20/20 [00:09<00:00,  2.11it/s]\n",
      "Images/Personal-Propic/o->117=4.24 - Octave: 1, Shape: (573, 573), Loss: [2.65, 18.34]: 100%|| 20/20 [00:20<00:00,  1.01s/it]\n",
      "Images/Personal-Propic/o->117=4.24 - Octave: 2, Shape: (860, 860), Loss: [2.66, 17.78]: 100%|| 20/20 [00:46<00:00,  2.32s/it]\n",
      "Images/Personal-Propic/o->192=6.46 - Octave: 0, Shape: (382, 382), Loss: [1.65, 16.25]: 100%|| 20/20 [00:12<00:00,  1.61it/s]\n",
      "Images/Personal-Propic/o->192=6.46 - Octave: 1, Shape: (573, 573), Loss: [2.57, 16.54]: 100%|| 20/20 [00:25<00:00,  1.29s/it]\n",
      "Images/Personal-Propic/o->192=6.46 - Octave: 2, Shape: (860, 860), Loss: [2.28, 15.71]: 100%|| 20/20 [00:57<00:00,  2.85s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73 - Octave: 0, Shape: (382, 382), Loss: [0.03, 2.31]: 100%|| 20/20 [00:17<00:00,  1.13it/s]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73 - Octave: 1, Shape: (573, 573), Loss: [0.07, 3.52]: 100%|| 20/20 [00:35<00:00,  1.78s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73 - Octave: 2, Shape: (860, 860), Loss: [0.07, 2.72]: 100%|| 20/20 [01:22<00:00,  4.12s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78 - Octave: 0, Shape: (382, 382), Loss: [3.47, 10.27]: 100%|| 20/20 [00:08<00:00,  2.27it/s]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78 - Octave: 1, Shape: (573, 573), Loss: [4.21, 10.93]: 100%|| 20/20 [00:18<00:00,  1.06it/s]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78 - Octave: 2, Shape: (860, 860), Loss: [4.21, 15.64]: 100%|| 20/20 [00:41<00:00,  2.07s/it]\n",
      "Images/Personal-Propic/o->47=5.45->70=8.12 - Octave: 0, Shape: (382, 382), Loss: [11.23, 21.42]:  20%|        | 4/20 [00:02<00:09,  1.63it/s]\n",
      "Images/Personal-Propic/o->47=5.45->70=8.12 - Octave: 1, Shape: (573, 573), Loss: [11.52, 21.87]:  20%|        | 4/20 [00:04<00:18,  1.18s/it]\n",
      "Images/Personal-Propic/o->47=5.45->70=8.12 - Octave: 2, Shape: (860, 860), Loss: [11.81, 22.00]:  20%|        | 4/20 [00:10<00:41,  2.57s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8 - Octave: 0, Shape: (382, 382), Loss: [7.66, 20.57]:  60%|    | 12/20 [00:05<00:03,  2.03it/s]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8 - Octave: 1, Shape: (573, 573), Loss: [7.12, 20.56]:  60%|    | 12/20 [00:12<00:08,  1.03s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8 - Octave: 2, Shape: (860, 860), Loss: [6.76, 20.42]:  55%|    | 11/20 [00:24<00:20,  2.27s/it]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48 - Octave: 0, Shape: (382, 382), Loss: [6.11, 20.69]:  25%|       | 5/20 [00:04<00:13,  1.08it/s]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48 - Octave: 1, Shape: (573, 573), Loss: [6.04, 20.08]:  25%|       | 5/20 [00:08<00:25,  1.70s/it]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48 - Octave: 2, Shape: (860, 860), Loss: [5.41, 21.02]:  30%|       | 6/20 [00:20<00:47,  3.39s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68 - Octave: 0, Shape: (382, 382), Loss: [5.02, 25.12]:  55%|    | 11/20 [00:03<00:02,  3.31it/s]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68 - Octave: 1, Shape: (573, 573), Loss: [13.52, 23.60]:   5%|         | 1/20 [00:01<00:25,  1.36s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68 - Octave: 2, Shape: (860, 860), Loss: [22.29, 22.29]:   0%|          | 0/20 [00:01<?, ?it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57 - Octave: 0, Shape: (382, 382), Loss: [3.96, 20.61]:  95%|| 19/20 [00:08<00:00,  2.28it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57 - Octave: 1, Shape: (573, 573), Loss: [4.33, 20.82]:  80%|  | 16/20 [00:14<00:03,  1.13it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57 - Octave: 2, Shape: (860, 860), Loss: [4.01, 20.83]:  80%|  | 16/20 [00:31<00:07,  1.97s/it]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4 - Octave: 0, Shape: (382, 382), Loss: [3.55, 21.01]:  35%|      | 7/20 [00:07<00:14,  1.08s/it]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4 - Octave: 1, Shape: (573, 573), Loss: [3.99, 21.13]:  45%|     | 9/20 [00:17<00:21,  1.95s/it]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4 - Octave: 2, Shape: (860, 860), Loss: [3.50, 22.10]:  55%|    | 11/20 [00:43<00:35,  3.98s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81 - Octave: 0, Shape: (382, 382), Loss: [11.82, 20.03]:  10%|         | 2/20 [00:01<00:17,  1.01it/s]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81 - Octave: 1, Shape: (573, 573), Loss: [11.60, 22.12]:  15%|        | 3/20 [00:05<00:29,  1.71s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81 - Octave: 2, Shape: (860, 860), Loss: [11.21, 21.39]:  15%|        | 3/20 [00:08<00:50,  3.00s/it]\n",
      "Images/Personal-Propic/o->172=6.03->177=1.18 - Octave: 0, Shape: (382, 382), Loss: [0.73, 8.05]: 100%|| 20/20 [00:11<00:00,  1.68it/s]\n",
      "Images/Personal-Propic/o->172=6.03->177=1.18 - Octave: 1, Shape: (573, 573), Loss: [0.51, 14.26]: 100%|| 20/20 [00:25<00:00,  1.27s/it]\n",
      "Images/Personal-Propic/o->172=6.03->177=1.18 - Octave: 2, Shape: (860, 860), Loss: [0.45, 16.45]: 100%|| 20/20 [00:57<00:00,  2.86s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4 - Octave: 0, Shape: (382, 382), Loss: [6.96, 29.08]:  10%|         | 2/20 [00:04<00:36,  2.02s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4 - Octave: 1, Shape: (573, 573), Loss: [8.08, 28.71]:  10%|         | 2/20 [00:06<00:58,  3.24s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4 - Octave: 2, Shape: (860, 860), Loss: [9.70, 20.68]:   5%|         | 1/20 [00:08<02:46,  8.75s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]WARNING: Logging before flag parsing goes to stderr.\n",
      "W0603 18:07:36.818202 4814732736 def_function.py:474] 5 out of the last 29 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43 - Octave: 0, Shape: (382, 382), Loss: [2.55, 22.33]:  25%|       | 5/20 [00:08<00:24,  1.62s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43 - Octave: 1, Shape: (573, 573), Loss: [3.21, 20.29]:  35%|      | 7/20 [00:16<00:29,  2.31s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43 - Octave: 2, Shape: (860, 860), Loss: [2.77, 20.37]:  45%|     | 9/20 [00:39<00:47,  4.34s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15 - Octave: 0, Shape: (382, 382), Loss: [12.94, 23.08]:   5%|         | 1/20 [00:01<00:34,  1.83s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15 - Octave: 1, Shape: (573, 573), Loss: [11.71, 21.23]:   5%|         | 1/20 [00:02<00:55,  2.93s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15 - Octave: 2, Shape: (860, 860), Loss: [11.80, 20.57]:   5%|         | 1/20 [00:05<01:45,  5.54s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:08:47.284060 4814732736 def_function.py:474] 5 out of the last 17 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65 - Octave: 0, Shape: (382, 382), Loss: [15.11, 20.07]:  10%|         | 2/20 [00:00<00:08,  2.02it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:08:48.638914 4814732736 def_function.py:474] 6 out of the last 20 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65 - Octave: 1, Shape: (573, 573), Loss: [14.30, 21.30]:  15%|        | 3/20 [00:02<00:14,  1.16it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:08:52.010799 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65 - Octave: 2, Shape: (860, 860), Loss: [14.68, 21.61]:  15%|        | 3/20 [00:05<00:31,  1.87s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74 - Octave: 0, Shape: (382, 382), Loss: [18.40, 25.57]:   5%|         | 1/20 [00:00<00:13,  1.39it/s]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74 - Octave: 1, Shape: (573, 573), Loss: [18.05, 25.01]:   5%|         | 1/20 [00:01<00:26,  1.37s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:09:00.007790 4814732736 def_function.py:474] 5 out of the last 13 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74 - Octave: 2, Shape: (860, 860), Loss: [16.93, 23.23]:   5%|         | 1/20 [00:02<00:55,  2.90s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:09:04.078789 4814732736 def_function.py:474] 5 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56 - Octave: 0, Shape: (382, 382), Loss: [0.68, 7.62]: 100%|| 20/20 [00:13<00:00,  1.49it/s]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56 - Octave: 1, Shape: (573, 573), Loss: [0.81, 6.57]: 100%|| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56 - Octave: 2, Shape: (860, 860), Loss: [0.66, 5.30]: 100%|| 20/20 [01:04<00:00,  3.22s/it]\n",
      "Images/Personal-Propic/o->290=1.5 - Octave: 0, Shape: (382, 382), Loss: [0.01, 0.43]: 100%|| 20/20 [00:16<00:00,  1.18it/s]\n",
      "Images/Personal-Propic/o->290=1.5 - Octave: 1, Shape: (573, 573), Loss: [0.02, 0.57]: 100%|| 20/20 [00:35<00:00,  1.76s/it]\n",
      "Images/Personal-Propic/o->290=1.5 - Octave: 2, Shape: (860, 860), Loss: [0.02, 0.64]: 100%|| 20/20 [01:17<00:00,  3.85s/it]\n",
      "Images/Personal-Propic/o->128=2.22 - Octave: 0, Shape: (382, 382), Loss: [1.36, 10.25]: 100%|| 20/20 [00:10<00:00,  1.95it/s]\n",
      "Images/Personal-Propic/o->128=2.22 - Octave: 1, Shape: (573, 573), Loss: [2.02, 12.15]: 100%|| 20/20 [00:21<00:00,  1.07s/it]\n",
      "Images/Personal-Propic/o->128=2.22 - Octave: 2, Shape: (860, 860), Loss: [1.97, 12.28]: 100%|| 20/20 [00:47<00:00,  2.39s/it]\n",
      "Images/Personal-Propic/o->128=3.86 - Octave: 0, Shape: (382, 382), Loss: [2.36, 17.79]: 100%|| 20/20 [00:10<00:00,  1.95it/s]\n",
      "Images/Personal-Propic/o->128=3.86 - Octave: 1, Shape: (573, 573), Loss: [3.49, 20.37]:  90%| | 18/20 [00:20<00:02,  1.14s/it]\n",
      "Images/Personal-Propic/o->128=3.86 - Octave: 2, Shape: (860, 860), Loss: [3.38, 20.25]:  90%| | 18/20 [00:45<00:05,  2.52s/it]\n",
      "Images/Personal-Propic/o->53=9.03 - Octave: 0, Shape: (382, 382), Loss: [5.51, 20.03]:  25%|       | 5/20 [00:02<00:07,  1.95it/s]\n",
      "Images/Personal-Propic/o->53=9.03 - Octave: 1, Shape: (573, 573), Loss: [7.57, 21.02]:  25%|       | 5/20 [00:05<00:15,  1.00s/it]\n",
      "Images/Personal-Propic/o->53=9.03 - Octave: 2, Shape: (860, 860), Loss: [7.50, 22.09]:  30%|       | 6/20 [00:12<00:29,  2.08s/it]\n",
      "Images/Personal-Propic/o->128=2.22->42=6.67 - Octave: 0, Shape: (382, 382), Loss: [13.95, 22.96]:  10%|         | 2/20 [00:01<00:12,  1.43it/s]\n",
      "Images/Personal-Propic/o->128=2.22->42=6.67 - Octave: 1, Shape: (573, 573), Loss: [13.06, 22.08]:  10%|         | 2/20 [00:02<00:23,  1.28s/it]\n",
      "Images/Personal-Propic/o->128=2.22->42=6.67 - Octave: 2, Shape: (860, 860), Loss: [13.97, 23.68]:  10%|         | 2/20 [00:05<00:47,  2.63s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:16:04.775216 4814732736 def_function.py:474] 5 out of the last 17 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->128=2.22->31=6.71 - Octave: 0, Shape: (382, 382), Loss: [18.66, 25.72]:   5%|         | 1/20 [00:00<00:14,  1.35it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:16:05.880752 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->128=2.22->31=6.71 - Octave: 1, Shape: (573, 573), Loss: [17.57, 23.81]:   5%|         | 1/20 [00:01<00:27,  1.43s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:16:08.161298 4814732736 def_function.py:474] 5 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->128=2.22->31=6.71 - Octave: 2, Shape: (860, 860), Loss: [16.31, 21.99]:   5%|         | 1/20 [00:03<00:57,  3.01s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:16:14.556016 4814732736 def_function.py:474] 6 out of the last 13 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->128=2.22->257=2.1 - Octave: 0, Shape: (382, 382), Loss: [0.21, 5.63]: 100%|| 20/20 [00:17<00:00,  1.12it/s]\n",
      "Images/Personal-Propic/o->128=2.22->257=2.1 - Octave: 1, Shape: (573, 573), Loss: [0.21, 5.16]: 100%|| 20/20 [00:34<00:00,  1.71s/it]\n",
      "Images/Personal-Propic/o->128=2.22->257=2.1 - Octave: 2, Shape: (860, 860), Loss: [0.17, 3.84]: 100%|| 20/20 [01:14<00:00,  3.74s/it]\n",
      "Images/Personal-Propic/o->128=2.22->57=1.29 - Octave: 0, Shape: (382, 382), Loss: [3.07, 18.42]: 100%|| 20/20 [00:07<00:00,  2.65it/s]\n",
      "Images/Personal-Propic/o->128=2.22->57=1.29 - Octave: 1, Shape: (573, 573), Loss: [3.78, 18.37]: 100%|| 20/20 [00:15<00:00,  1.26it/s]\n",
      "Images/Personal-Propic/o->128=2.22->57=1.29 - Octave: 2, Shape: (860, 860), Loss: [3.43, 17.84]: 100%|| 20/20 [00:34<00:00,  1.74s/it]\n",
      "Images/Personal-Propic/o->53=9.03->99=6.34 - Octave: 0, Shape: (382, 382), Loss: [13.26, 20.30]:  10%|         | 2/20 [00:02<00:18,  1.05s/it]\n",
      "Images/Personal-Propic/o->53=9.03->99=6.34 - Octave: 1, Shape: (573, 573), Loss: [11.33, 22.21]:  20%|        | 4/20 [00:05<00:21,  1.37s/it]\n",
      "Images/Personal-Propic/o->53=9.03->99=6.34 - Octave: 2, Shape: (860, 860), Loss: [10.59, 21.18]:  20%|        | 4/20 [00:11<00:45,  2.85s/it]\n",
      "Images/Personal-Propic/o->53=9.03->53=9.59 - Octave: 0, Shape: (382, 382), Loss: [19.08, 25.01]:   5%|         | 1/20 [00:01<00:21,  1.16s/it]\n",
      "Images/Personal-Propic/o->53=9.03->53=9.59 - Octave: 1, Shape: (573, 573), Loss: [15.48, 20.65]:   5%|         | 1/20 [00:01<00:37,  1.97s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:19:40.264168 4814732736 def_function.py:474] 5 out of the last 15 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->53=9.03->53=9.59 - Octave: 2, Shape: (860, 860), Loss: [15.28, 20.15]:   5%|         | 1/20 [00:03<01:14,  3.91s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:19:43.706645 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->53=9.03->121=6.53 - Octave: 0, Shape: (382, 382), Loss: [5.90, 22.26]:  20%|        | 4/20 [00:03<00:13,  1.19it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:19:47.657205 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->53=9.03->121=6.53 - Octave: 1, Shape: (573, 573), Loss: [7.36, 20.44]:  15%|        | 3/20 [00:05<00:29,  1.72s/it]\n",
      "Images/Personal-Propic/o->53=9.03->121=6.53 - Octave: 2, Shape: (860, 860), Loss: [7.22, 21.60]:  20%|        | 4/20 [00:12<00:51,  3.23s/it]\n",
      "Images/Personal-Propic/o->53=9.03->84=6.35 - Octave: 0, Shape: (382, 382), Loss: [1.44, 10.11]: 100%|| 20/20 [00:08<00:00,  2.30it/s]\n",
      "Images/Personal-Propic/o->53=9.03->84=6.35 - Octave: 1, Shape: (573, 573), Loss: [2.23, 10.49]: 100%|| 20/20 [00:18<00:00,  1.09it/s]\n",
      "Images/Personal-Propic/o->53=9.03->84=6.35 - Octave: 2, Shape: (860, 860), Loss: [2.15, 10.01]: 100%|| 20/20 [00:40<00:00,  2.01s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->180=6.24 - Octave: 0, Shape: (382, 382), Loss: [6.05, 24.97]:  10%|         | 2/20 [00:03<00:30,  1.67s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->180=6.24 - Octave: 1, Shape: (573, 573), Loss: [6.74, 24.49]:  10%|         | 2/20 [00:05<00:49,  2.73s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->180=6.24 - Octave: 2, Shape: (860, 860), Loss: [5.68, 23.53]:  10%|         | 2/20 [00:10<01:32,  5.16s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:21:32.815658 4814732736 def_function.py:474] 5 out of the last 30 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->143=3.38 - Octave: 0, Shape: (382, 382), Loss: [1.42, 8.99]: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->143=3.38 - Octave: 1, Shape: (573, 573), Loss: [1.52, 8.96]: 100%|| 20/20 [00:23<00:00,  1.19s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->143=3.38 - Octave: 2, Shape: (860, 860), Loss: [1.28, 8.12]: 100%|| 20/20 [00:52<00:00,  2.63s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->148=6.75 - Octave: 0, Shape: (382, 382), Loss: [10.24, 22.43]:   5%|         | 1/20 [00:04<01:22,  4.36s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->148=6.75 - Octave: 1, Shape: (573, 573), Loss: [9.99, 22.65]:   5%|         | 1/20 [00:03<01:09,  3.65s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->148=6.75 - Octave: 2, Shape: (860, 860), Loss: [9.18, 21.55]:   5%|         | 1/20 [00:06<02:05,  6.62s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:23:16.684879 4814732736 def_function.py:474] 5 out of the last 27 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->227=3.17 - Octave: 0, Shape: (382, 382), Loss: [0.04, 2.72]: 100%|| 20/20 [00:13<00:00,  1.43it/s]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->227=3.17 - Octave: 1, Shape: (573, 573), Loss: [0.06, 2.83]: 100%|| 20/20 [00:29<00:00,  1.45s/it]\n",
      "Images/Personal-Propic/o->117=4.24->31=7.74->227=3.17 - Octave: 2, Shape: (860, 860), Loss: [0.05, 2.34]: 100%|| 20/20 [01:04<00:00,  3.22s/it]\n",
      "Images/Personal-Propic/o->47=5.45->69=8.21 - Octave: 0, Shape: (382, 382), Loss: [20.31, 20.31]:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Images/Personal-Propic/o->47=5.45->69=8.21 - Octave: 1, Shape: (573, 573), Loss: [15.63, 20.55]:   5%|         | 1/20 [00:02<00:43,  2.29s/it]\n",
      "Images/Personal-Propic/o->47=5.45->69=8.21 - Octave: 2, Shape: (860, 860), Loss: [14.28, 22.02]:  10%|         | 2/20 [00:06<00:56,  3.15s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:25:14.068899 4814732736 def_function.py:474] 5 out of the last 27 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->169=0.97 - Octave: 0, Shape: (382, 382), Loss: [1.00, 14.55]: 100%|| 20/20 [00:12<00:00,  1.54it/s]\n",
      "Images/Personal-Propic/o->47=5.45->169=0.97 - Octave: 1, Shape: (573, 573), Loss: [1.35, 21.01]:  80%|  | 16/20 [00:23<00:05,  1.45s/it]\n",
      "Images/Personal-Propic/o->47=5.45->169=0.97 - Octave: 2, Shape: (860, 860), Loss: [1.16, 20.13]:  70%|   | 14/20 [00:45<00:19,  3.28s/it]\n",
      "Images/Personal-Propic/o->47=5.45->163=8.38 - Octave: 0, Shape: (382, 382), Loss: [1.63, 22.55]:  35%|      | 7/20 [00:07<00:13,  1.04s/it]\n",
      "Images/Personal-Propic/o->47=5.45->163=8.38 - Octave: 1, Shape: (573, 573), Loss: [2.11, 23.10]:  30%|       | 6/20 [00:11<00:26,  1.87s/it]\n",
      "Images/Personal-Propic/o->47=5.45->163=8.38 - Octave: 2, Shape: (860, 860), Loss: [2.16, 22.69]:  30%|       | 6/20 [00:25<00:59,  4.26s/it]\n",
      "Images/Personal-Propic/o->47=5.45->95=0.96 - Octave: 0, Shape: (382, 382), Loss: [1.76, 6.73]: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n",
      "Images/Personal-Propic/o->47=5.45->95=0.96 - Octave: 1, Shape: (573, 573), Loss: [1.74, 7.25]: 100%|| 20/20 [00:21<00:00,  1.05s/it]\n",
      "Images/Personal-Propic/o->47=5.45->95=0.96 - Octave: 2, Shape: (860, 860), Loss: [1.64, 7.63]: 100%|| 20/20 [00:46<00:00,  2.30s/it]\n",
      "Images/Personal-Propic/o->192=6.46->98=4.42 - Octave: 0, Shape: (382, 382), Loss: [3.49, 21.38]:  35%|      | 7/20 [00:04<00:08,  1.57it/s]\n",
      "Images/Personal-Propic/o->192=6.46->98=4.42 - Octave: 1, Shape: (573, 573), Loss: [4.23, 21.49]:  35%|      | 7/20 [00:08<00:16,  1.25s/it]\n",
      "Images/Personal-Propic/o->192=6.46->98=4.42 - Octave: 2, Shape: (860, 860), Loss: [3.71, 20.64]:  40%|      | 8/20 [00:20<00:31,  2.59s/it]\n",
      "Images/Personal-Propic/o->192=6.46->42=9.8 - Octave: 0, Shape: (382, 382), Loss: [16.51, 22.13]:   5%|         | 1/20 [00:01<00:21,  1.11s/it]\n",
      "Images/Personal-Propic/o->192=6.46->42=9.8 - Octave: 1, Shape: (573, 573), Loss: [15.18, 20.90]:   5%|         | 1/20 [00:01<00:35,  1.86s/it]\n",
      "Images/Personal-Propic/o->192=6.46->42=9.8 - Octave: 2, Shape: (860, 860), Loss: [15.60, 21.56]:   5%|         | 1/20 [00:03<01:10,  3.70s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:29:22.151026 4814732736 def_function.py:474] 5 out of the last 16 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->201=3.59 - Octave: 0, Shape: (382, 382), Loss: [8.84, 20.75]:  15%|        | 3/20 [00:04<00:24,  1.47s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:29:27.326992 4814732736 def_function.py:474] 5 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->201=3.59 - Octave: 1, Shape: (573, 573), Loss: [8.56, 20.29]:  20%|        | 4/20 [00:08<00:35,  2.20s/it]\n",
      "Images/Personal-Propic/o->192=6.46->201=3.59 - Octave: 2, Shape: (860, 860), Loss: [7.88, 20.91]:  30%|       | 6/20 [00:26<01:01,  4.37s/it]\n",
      "Images/Personal-Propic/o->192=6.46->0=4.81 - Octave: 0, Shape: (382, 382), Loss: [1.92, 3.45]: 100%|| 20/20 [00:00<00:00, 86.16it/s]\n",
      "Images/Personal-Propic/o->192=6.46->0=4.81 - Octave: 1, Shape: (573, 573), Loss: [3.47, 5.44]: 100%|| 20/20 [00:00<00:00, 51.72it/s]\n",
      "Images/Personal-Propic/o->192=6.46->0=4.81 - Octave: 2, Shape: (860, 860), Loss: [5.53, 7.97]: 100%|| 20/20 [00:00<00:00, 39.41it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->98=2.17 - Octave: 0, Shape: (382, 382), Loss: [1.84, 20.40]:  85%| | 17/20 [00:08<00:01,  1.89it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->98=2.17 - Octave: 1, Shape: (573, 573), Loss: [2.41, 20.70]:  85%| | 17/20 [00:18<00:03,  1.09s/it]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->98=2.17 - Octave: 2, Shape: (860, 860), Loss: [2.11, 19.64]: 100%|| 20/20 [00:45<00:00,  2.26s/it]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->62=5.65 - Octave: 0, Shape: (382, 382), Loss: [1.65, 20.62]:  60%|    | 12/20 [00:04<00:03,  2.44it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->62=5.65 - Octave: 1, Shape: (573, 573), Loss: [1.52, 21.05]:  80%|  | 16/20 [00:12<00:03,  1.24it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->62=5.65 - Octave: 2, Shape: (860, 860), Loss: [1.70, 21.07]:  70%|   | 14/20 [00:24<00:10,  1.77s/it]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->222=8.65 - Octave: 0, Shape: (382, 382), Loss: [8.80, 27.60]:  10%|         | 2/20 [00:03<00:35,  1.95s/it]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->222=8.65 - Octave: 1, Shape: (573, 573), Loss: [11.94, 27.32]:   5%|         | 1/20 [00:05<01:49,  5.76s/it]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->222=8.65 - Octave: 2, Shape: (860, 860), Loss: [11.09, 27.63]:   5%|         | 1/20 [00:11<03:31, 11.14s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:32:19.751749 4814732736 def_function.py:474] 5 out of the last 23 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->123=5.09 - Octave: 0, Shape: (382, 382), Loss: [4.02, 21.11]:  45%|     | 9/20 [00:06<00:07,  1.49it/s]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->123=5.09 - Octave: 1, Shape: (573, 573), Loss: [3.54, 21.46]:  45%|     | 9/20 [00:11<00:14,  1.29s/it]\n",
      "Images/Personal-Propic/o->192=6.46->72=9.57->123=5.09 - Octave: 2, Shape: (860, 860), Loss: [2.94, 20.88]:  50%|     | 10/20 [00:27<00:27,  2.71s/it]\n",
      "Images/Personal-Propic/o->172=6.03->270=0.83 - Octave: 0, Shape: (382, 382), Loss: [0.02, 1.11]: 100%|| 20/20 [00:16<00:00,  1.19it/s]\n",
      "Images/Personal-Propic/o->172=6.03->270=0.83 - Octave: 1, Shape: (573, 573), Loss: [0.06, 1.05]: 100%|| 20/20 [00:34<00:00,  1.73s/it]\n",
      "Images/Personal-Propic/o->172=6.03->270=0.83 - Octave: 2, Shape: (860, 860), Loss: [0.04, 1.03]: 100%|| 20/20 [01:20<00:00,  4.01s/it]\n",
      "Images/Personal-Propic/o->172=6.03->41=2.78 - Octave: 0, Shape: (382, 382), Loss: [8.24, 20.58]:  50%|     | 10/20 [00:04<00:04,  2.38it/s]\n",
      "Images/Personal-Propic/o->172=6.03->41=2.78 - Octave: 1, Shape: (573, 573), Loss: [11.53, 20.46]:  50%|     | 10/20 [00:08<00:08,  1.16it/s]\n",
      "Images/Personal-Propic/o->172=6.03->41=2.78 - Octave: 2, Shape: (860, 860), Loss: [11.42, 20.07]:  45%|     | 9/20 [00:17<00:21,  1.94s/it]\n",
      "Images/Personal-Propic/o->172=6.03->58=0.09 - Octave: 0, Shape: (382, 382), Loss: [0.16, 1.40]: 100%|| 20/20 [00:07<00:00,  2.62it/s]\n",
      "Images/Personal-Propic/o->172=6.03->58=0.09 - Octave: 1, Shape: (573, 573), Loss: [0.22, 3.20]: 100%|| 20/20 [00:15<00:00,  1.27it/s]\n",
      "Images/Personal-Propic/o->172=6.03->58=0.09 - Octave: 2, Shape: (860, 860), Loss: [0.28, 24.44]:  70%|   | 14/20 [00:25<00:10,  1.82s/it]\n",
      "Images/Personal-Propic/o->172=6.03->193=8.42 - Octave: 0, Shape: (382, 382), Loss: [1.67, 21.20]:  90%| | 18/20 [00:12<00:01,  1.42it/s]\n",
      "Images/Personal-Propic/o->172=6.03->193=8.42 - Octave: 1, Shape: (573, 573), Loss: [1.61, 20.52]:  65%|   | 13/20 [00:21<00:11,  1.63s/it]\n",
      "Images/Personal-Propic/o->172=6.03->193=8.42 - Octave: 2, Shape: (860, 860), Loss: [1.22, 20.77]:  60%|    | 12/20 [00:47<00:31,  3.94s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->80=2.65 - Octave: 0, Shape: (382, 382), Loss: [2.64, 20.12]:  80%|  | 16/20 [00:10<00:02,  1.58it/s]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->80=2.65 - Octave: 1, Shape: (573, 573), Loss: [3.92, 20.46]:  65%|   | 13/20 [00:16<00:08,  1.26s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->80=2.65 - Octave: 2, Shape: (860, 860), Loss: [3.83, 20.20]:  70%|   | 14/20 [00:31<00:13,  2.22s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->32=3.98 - Octave: 0, Shape: (382, 382), Loss: [23.82, 23.82]:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->32=3.98 - Octave: 1, Shape: (573, 573), Loss: [20.86, 20.86]:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->32=3.98 - Octave: 2, Shape: (860, 860), Loss: [21.09, 21.09]:   0%|          | 0/20 [00:01<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:39:00.504211 4814732736 def_function.py:474] 5 out of the last 19 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->182=5.53 - Octave: 0, Shape: (382, 382), Loss: [1.17, 20.62]:  95%|| 19/20 [00:13<00:00,  1.40it/s]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->182=5.53 - Octave: 1, Shape: (573, 573), Loss: [1.99, 20.59]:  70%|   | 14/20 [00:26<00:11,  1.92s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->182=5.53 - Octave: 2, Shape: (860, 860), Loss: [1.63, 20.73]:  70%|   | 14/20 [00:52<00:22,  3.75s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->128=1.65 - Octave: 0, Shape: (382, 382), Loss: [1.16, 8.12]: 100%|| 20/20 [00:16<00:00,  1.23it/s]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->128=1.65 - Octave: 1, Shape: (573, 573), Loss: [1.39, 8.99]: 100%|| 20/20 [00:28<00:00,  1.40s/it]\n",
      "Images/Personal-Propic/o->47=5.45->292=2.73->128=1.65 - Octave: 2, Shape: (860, 860), Loss: [1.26, 9.13]: 100%|| 20/20 [00:52<00:00,  2.64s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->77=5.7 - Octave: 0, Shape: (382, 382), Loss: [6.31, 20.63]:  65%|   | 13/20 [00:06<00:03,  1.98it/s]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->77=5.7 - Octave: 1, Shape: (573, 573), Loss: [8.26, 20.03]:  50%|     | 10/20 [00:10<00:10,  1.08s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->77=5.7 - Octave: 2, Shape: (860, 860), Loss: [8.28, 20.06]:  40%|      | 8/20 [00:19<00:29,  2.49s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->286=7.04 - Octave: 0, Shape: (382, 382), Loss: [16.04, 28.73]:   5%|         | 1/20 [00:04<01:28,  4.64s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->286=7.04 - Octave: 1, Shape: (573, 573), Loss: [11.26, 29.16]:  10%|         | 2/20 [00:08<01:16,  4.23s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->286=7.04 - Octave: 2, Shape: (860, 860), Loss: [15.00, 22.23]:   5%|         | 1/20 [00:11<03:37, 11.44s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:43:15.582967 4814732736 def_function.py:474] 5 out of the last 17 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->280=2.88 - Octave: 0, Shape: (382, 382), Loss: [1.91, 13.01]: 100%|| 20/20 [00:19<00:00,  1.05it/s]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->280=2.88 - Octave: 1, Shape: (573, 573), Loss: [1.08, 8.48]: 100%|| 20/20 [00:39<00:00,  1.97s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->280=2.88 - Octave: 2, Shape: (860, 860), Loss: [0.67, 8.62]: 100%|| 20/20 [01:25<00:00,  4.29s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->125=4.33 - Octave: 0, Shape: (382, 382), Loss: [5.15, 20.06]:  15%|        | 3/20 [00:03<00:19,  1.14s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->125=4.33 - Octave: 1, Shape: (573, 573), Loss: [6.46, 23.10]:  15%|        | 3/20 [00:06<00:34,  2.00s/it]\n",
      "Images/Personal-Propic/o->172=6.03->285=1.43->125=4.33 - Octave: 2, Shape: (860, 860), Loss: [6.46, 22.50]:  15%|        | 3/20 [00:11<01:07,  3.96s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->19=8.92 - Octave: 0, Shape: (382, 382), Loss: [27.50, 27.50]:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:45:59.554156 4814732736 def_function.py:474] 5 out of the last 14 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->19=8.92 - Octave: 1, Shape: (573, 573), Loss: [19.69, 27.05]:   5%|         | 1/20 [00:01<00:30,  1.59s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:46:01.997488 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->19=8.92 - Octave: 2, Shape: (860, 860), Loss: [18.47, 25.12]:   5%|         | 1/20 [00:03<01:01,  3.25s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:46:05.261940 4814732736 def_function.py:474] 6 out of the last 14 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->95=6.8 - Octave: 0, Shape: (382, 382), Loss: [17.67, 22.83]:   5%|         | 1/20 [00:01<00:36,  1.92s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:46:07.838156 4814732736 def_function.py:474] 6 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->95=6.8 - Octave: 1, Shape: (573, 573), Loss: [15.55, 20.45]:   5%|         | 1/20 [00:03<00:59,  3.11s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:46:12.320197 4814732736 def_function.py:474] 7 out of the last 14 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->95=6.8 - Octave: 2, Shape: (860, 860), Loss: [15.27, 20.21]:   5%|         | 1/20 [00:05<01:51,  5.87s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:46:18.631794 4814732736 def_function.py:474] 6 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->248=4.49 - Octave: 0, Shape: (382, 382), Loss: [0.51, 18.95]: 100%|| 20/20 [00:18<00:00,  1.11it/s]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->248=4.49 - Octave: 1, Shape: (573, 573), Loss: [0.52, 14.37]: 100%|| 20/20 [00:37<00:00,  1.87s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->248=4.49 - Octave: 2, Shape: (860, 860), Loss: [0.48, 12.76]: 100%|| 20/20 [01:20<00:00,  4.03s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->180=9.79 - Octave: 0, Shape: (382, 382), Loss: [7.97, 34.21]:  10%|         | 2/20 [00:03<00:33,  1.84s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->180=9.79 - Octave: 1, Shape: (573, 573), Loss: [8.53, 20.26]:   5%|         | 1/20 [00:04<01:29,  4.69s/it]\n",
      "Images/Personal-Propic/o->47=5.45->88=4.8->180=9.79 - Octave: 2, Shape: (860, 860), Loss: [7.48, 33.40]:  10%|         | 2/20 [00:11<01:42,  5.68s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:48:53.751057 4814732736 def_function.py:474] 5 out of the last 29 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->139=2.5 - Octave: 0, Shape: (382, 382), Loss: [1.60, 13.07]: 100%|| 20/20 [00:12<00:00,  1.59it/s]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->139=2.5 - Octave: 1, Shape: (573, 573), Loss: [1.80, 14.11]: 100%|| 20/20 [00:26<00:00,  1.33s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->139=2.5 - Octave: 2, Shape: (860, 860), Loss: [1.59, 18.74]: 100%|| 20/20 [00:57<00:00,  2.90s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->86=3.13 - Octave: 0, Shape: (382, 382), Loss: [2.24, 6.96]: 100%|| 20/20 [00:10<00:00,  2.00it/s]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->86=3.13 - Octave: 1, Shape: (573, 573), Loss: [2.08, 7.12]: 100%|| 20/20 [00:21<00:00,  1.08s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->86=3.13 - Octave: 2, Shape: (860, 860), Loss: [1.92, 7.60]: 100%|| 20/20 [00:47<00:00,  2.37s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->109=9.65 - Octave: 0, Shape: (382, 382), Loss: [18.94, 29.27]:   5%|         | 1/20 [00:02<00:42,  2.25s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->109=9.65 - Octave: 1, Shape: (573, 573), Loss: [18.30, 29.28]:   5%|         | 1/20 [00:03<01:03,  3.36s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->109=9.65 - Octave: 2, Shape: (860, 860), Loss: [18.25, 29.06]:   5%|         | 1/20 [00:06<01:57,  6.18s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:52:07.130586 4814732736 def_function.py:474] 5 out of the last 27 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->184=5.88 - Octave: 0, Shape: (382, 382), Loss: [6.03, 22.34]:  20%|        | 4/20 [00:08<00:35,  2.20s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 18:52:12.548323 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->184=5.88 - Octave: 1, Shape: (573, 573), Loss: [6.02, 22.67]:  20%|        | 4/20 [00:08<00:33,  2.11s/it]\n",
      "Images/Personal-Propic/o->172=6.03->99=7.81->184=5.88 - Octave: 2, Shape: (860, 860), Loss: [5.07, 20.64]:  20%|        | 4/20 [00:17<01:09,  4.33s/it]\n",
      "Images/Personal-Propic/o->117=4.24->117=6.06 - Octave: 0, Shape: (382, 382), Loss: [11.25, 22.22]:  15%|        | 3/20 [00:03<00:17,  1.05s/it]\n",
      "Images/Personal-Propic/o->117=4.24->117=6.06 - Octave: 1, Shape: (573, 573), Loss: [9.97, 21.58]:  20%|        | 4/20 [00:06<00:27,  1.72s/it]\n",
      "Images/Personal-Propic/o->117=4.24->117=6.06 - Octave: 2, Shape: (860, 860), Loss: [10.29, 20.58]:  20%|        | 4/20 [00:14<00:56,  3.54s/it]\n",
      "Images/Personal-Propic/o->117=4.24->83=0.19 - Octave: 0, Shape: (382, 382), Loss: [0.18, 0.63]: 100%|| 20/20 [00:08<00:00,  2.29it/s]\n",
      "Images/Personal-Propic/o->117=4.24->83=0.19 - Octave: 1, Shape: (573, 573), Loss: [0.18, 0.57]: 100%|| 20/20 [00:18<00:00,  1.08it/s]\n",
      "Images/Personal-Propic/o->117=4.24->83=0.19 - Octave: 2, Shape: (860, 860), Loss: [0.16, 0.55]: 100%|| 20/20 [00:40<00:00,  2.01s/it]\n",
      "Images/Personal-Propic/o->117=4.24->161=3.02 - Octave: 0, Shape: (382, 382), Loss: [0.81, 10.11]: 100%|| 20/20 [00:12<00:00,  1.64it/s]\n",
      "Images/Personal-Propic/o->117=4.24->161=3.02 - Octave: 1, Shape: (573, 573), Loss: [1.01, 10.78]: 100%|| 20/20 [00:26<00:00,  1.35s/it]\n",
      "Images/Personal-Propic/o->117=4.24->161=3.02 - Octave: 2, Shape: (860, 860), Loss: [0.87, 9.90]: 100%|| 20/20 [00:59<00:00,  2.96s/it]\n",
      "Images/Personal-Propic/o->117=4.24->228=6.6 - Octave: 0, Shape: (382, 382), Loss: [0.23, 3.31]: 100%|| 20/20 [00:17<00:00,  1.17it/s]\n",
      "Images/Personal-Propic/o->117=4.24->228=6.6 - Octave: 1, Shape: (573, 573), Loss: [0.25, 3.98]: 100%|| 20/20 [00:35<00:00,  1.78s/it]\n",
      "Images/Personal-Propic/o->117=4.24->228=6.6 - Octave: 2, Shape: (860, 860), Loss: [0.21, 3.16]: 100%|| 20/20 [01:17<00:00,  3.87s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->184=4.29 - Octave: 0, Shape: (382, 382), Loss: [4.67, 22.01]:  35%|      | 7/20 [00:07<00:13,  1.01s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->184=4.29 - Octave: 1, Shape: (573, 573), Loss: [4.22, 20.33]:  30%|       | 6/20 [00:11<00:27,  1.94s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->184=4.29 - Octave: 2, Shape: (860, 860), Loss: [3.63, 20.50]:  35%|      | 7/20 [00:26<00:50,  3.85s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->152=1.35 - Octave: 0, Shape: (382, 382), Loss: [1.92, 9.93]: 100%|| 20/20 [00:12<00:00,  1.57it/s]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->152=1.35 - Octave: 1, Shape: (573, 573), Loss: [1.69, 10.15]: 100%|| 20/20 [00:26<00:00,  1.33s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->152=1.35 - Octave: 2, Shape: (860, 860), Loss: [1.41, 10.91]: 100%|| 20/20 [00:57<00:00,  2.90s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->79=2.98 - Octave: 0, Shape: (382, 382), Loss: [8.10, 20.01]:  25%|       | 5/20 [00:03<00:10,  1.50it/s]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->79=2.98 - Octave: 1, Shape: (573, 573), Loss: [7.33, 20.33]:  35%|      | 7/20 [00:08<00:15,  1.21s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->79=2.98 - Octave: 2, Shape: (860, 860), Loss: [6.69, 20.18]:  40%|      | 8/20 [00:20<00:30,  2.50s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->41=5.7 - Octave: 0, Shape: (382, 382), Loss: [21.94, 21.94]:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->41=5.7 - Octave: 1, Shape: (573, 573), Loss: [19.99, 23.93]:   5%|         | 1/20 [00:02<00:38,  2.04s/it]\n",
      "Images/Personal-Propic/o->117=4.24->32=2.65->41=5.7 - Octave: 2, Shape: (860, 860), Loss: [21.17, 21.17]:   0%|          | 0/20 [00:02<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:01:00.440969 4814732736 def_function.py:474] 5 out of the last 14 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->129=6.52 - Octave: 0, Shape: (382, 382), Loss: [1.37, 20.39]:  70%|   | 14/20 [00:11<00:04,  1.24it/s]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->129=6.52 - Octave: 1, Shape: (573, 573), Loss: [2.50, 21.06]:  50%|     | 10/20 [00:19<00:19,  1.92s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->129=6.52 - Octave: 2, Shape: (860, 860), Loss: [2.36, 20.81]:  50%|     | 10/20 [00:32<00:32,  3.25s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->223=4.31 - Octave: 0, Shape: (382, 382), Loss: [10.32, 24.21]:  10%|         | 2/20 [00:04<00:38,  2.12s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->223=4.31 - Octave: 1, Shape: (573, 573), Loss: [13.56, 24.94]:   5%|         | 1/20 [00:05<01:36,  5.08s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->223=4.31 - Octave: 2, Shape: (860, 860), Loss: [19.88, 34.79]:   5%|         | 1/20 [00:08<02:48,  8.85s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:02:24.612906 4814732736 def_function.py:474] 5 out of the last 19 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->300=8.97 - Octave: 0, Shape: (382, 382), Loss: [9.50, 27.35]:  10%|         | 2/20 [00:05<00:47,  2.66s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:02:30.893509 4814732736 def_function.py:474] 5 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->300=8.97 - Octave: 1, Shape: (573, 573), Loss: [6.82, 21.45]:  15%|        | 3/20 [00:10<00:58,  3.43s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:02:43.938590 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->300=8.97 - Octave: 2, Shape: (860, 860), Loss: [6.51, 20.58]:  25%|       | 5/20 [00:28<01:24,  5.62s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->216=3.68 - Octave: 0, Shape: (382, 382), Loss: [3.47, 20.52]:  60%|    | 12/20 [00:11<00:07,  1.09it/s]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->216=3.68 - Octave: 1, Shape: (573, 573), Loss: [3.59, 20.90]:  55%|    | 11/20 [00:20<00:16,  1.86s/it]\n",
      "Images/Personal-Propic/o->172=6.03->243=6.4->216=3.68 - Octave: 2, Shape: (860, 860), Loss: [3.46, 21.16]:  55%|    | 11/20 [00:43<00:35,  3.94s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->291=7.04 - Octave: 0, Shape: (382, 382), Loss: [0.09, 3.68]: 100%|| 20/20 [00:20<00:00,  1.00s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->291=7.04 - Octave: 1, Shape: (573, 573), Loss: [0.14, 4.28]: 100%|| 20/20 [00:41<00:00,  2.08s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->291=7.04 - Octave: 2, Shape: (860, 860), Loss: [0.13, 4.41]: 100%|| 20/20 [02:05<00:00,  6.26s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->214=1.0 - Octave: 0, Shape: (382, 382), Loss: [0.10, 4.40]: 100%|| 20/20 [00:23<00:00,  1.15s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->214=1.0 - Octave: 1, Shape: (573, 573), Loss: [0.25, 4.21]: 100%|| 20/20 [00:44<00:00,  2.25s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->214=1.0 - Octave: 2, Shape: (860, 860), Loss: [0.18, 3.63]: 100%|| 20/20 [01:20<00:00,  4.03s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->189=9.19 - Octave: 0, Shape: (382, 382), Loss: [14.49, 30.89]:   5%|         | 1/20 [00:03<01:00,  3.17s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->189=9.19 - Octave: 1, Shape: (573, 573), Loss: [17.68, 34.48]:   5%|         | 1/20 [00:04<01:29,  4.69s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->189=9.19 - Octave: 2, Shape: (860, 860), Loss: [17.79, 32.66]:   5%|         | 1/20 [00:08<02:39,  8.39s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:10:16.587962 4814732736 def_function.py:474] 5 out of the last 27 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->197=7.14 - Octave: 0, Shape: (382, 382), Loss: [2.76, 20.08]:  60%|    | 12/20 [00:15<00:10,  1.28s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->197=7.14 - Octave: 1, Shape: (573, 573), Loss: [3.59, 20.98]:  60%|    | 12/20 [00:31<00:21,  2.66s/it]\n",
      "Images/Personal-Propic/o->192=6.46->25=3.68->197=7.14 - Octave: 2, Shape: (860, 860), Loss: [3.09, 20.19]:  70%|   | 14/20 [01:11<00:30,  5.08s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->125=5.08 - Octave: 0, Shape: (382, 382), Loss: [20.37, 20.37]:   0%|          | 0/20 [00:01<?, ?it/s]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->125=5.08 - Octave: 1, Shape: (573, 573), Loss: [18.47, 34.40]:   5%|         | 1/20 [00:04<01:16,  4.01s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->125=5.08 - Octave: 2, Shape: (860, 860), Loss: [18.82, 33.53]:   5%|         | 1/20 [00:06<01:59,  6.29s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:12:26.486649 4814732736 def_function.py:474] 5 out of the last 21 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->156=1.67 - Octave: 0, Shape: (382, 382), Loss: [3.46, 20.52]:  95%|| 19/20 [00:13<00:00,  1.45it/s]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->156=1.67 - Octave: 1, Shape: (573, 573), Loss: [3.34, 20.05]:  80%|  | 16/20 [00:21<00:05,  1.37s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->156=1.67 - Octave: 2, Shape: (860, 860), Loss: [2.87, 21.86]:  60%|    | 12/20 [00:36<00:24,  3.07s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->111=7.79 - Octave: 0, Shape: (382, 382), Loss: [5.14, 20.29]:  45%|     | 9/20 [00:06<00:07,  1.46it/s]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->111=7.79 - Octave: 1, Shape: (573, 573), Loss: [4.77, 20.45]:  65%|   | 13/20 [00:16<00:08,  1.26s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->111=7.79 - Octave: 2, Shape: (860, 860), Loss: [3.94, 20.26]:  80%|  | 16/20 [00:42<00:10,  2.64s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->258=8.65 - Octave: 0, Shape: (382, 382), Loss: [2.33, 20.62]:  90%| | 18/20 [00:16<00:01,  1.07it/s]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->258=8.65 - Octave: 1, Shape: (573, 573), Loss: [1.53, 18.62]: 100%|| 20/20 [00:35<00:00,  1.79s/it]\n",
      "Images/Personal-Propic/o->117=4.24->202=4.56->258=8.65 - Octave: 2, Shape: (860, 860), Loss: [1.44, 15.06]: 100%|| 20/20 [01:23<00:00,  4.18s/it]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->29=0.69 - Octave: 0, Shape: (382, 382), Loss: [4.50, 31.65]:  45%|     | 9/20 [00:03<00:04,  2.65it/s]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->29=0.69 - Octave: 1, Shape: (573, 573), Loss: [32.14, 32.14]:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->29=0.69 - Octave: 2, Shape: (860, 860), Loss: [44.76, 44.76]:   0%|          | 0/20 [00:01<?, ?it/s]\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->67=6.97 - Octave: 0, Shape: (382, 382), Loss: [15.97, 20.40]:   5%|         | 1/20 [00:01<00:27,  1.45s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:07.642720 4814732736 def_function.py:474] 5 out of the last 15 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->67=6.97 - Octave: 1, Shape: (573, 573), Loss: [14.67, 22.01]:  10%|         | 2/20 [00:03<00:28,  1.58s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:11.893055 4814732736 def_function.py:474] 6 out of the last 18 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->67=6.97 - Octave: 2, Shape: (860, 860), Loss: [14.71, 22.14]:  10%|         | 2/20 [00:06<00:57,  3.19s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:16.533626 4814732736 def_function.py:474] 6 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->35=4.54 - Octave: 0, Shape: (382, 382), Loss: [10.38, 22.15]:   5%|         | 1/20 [00:00<00:15,  1.20it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:17.703747 4814732736 def_function.py:474] 5 out of the last 11 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->35=4.54 - Octave: 1, Shape: (573, 573), Loss: [7.77, 28.16]:  10%|         | 2/20 [00:02<00:19,  1.08s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:20.693346 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->35=4.54 - Octave: 2, Shape: (860, 860), Loss: [7.80, 25.20]:  10%|         | 2/20 [00:04<00:40,  2.25s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:27.418426 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->295=7.22 - Octave: 0, Shape: (382, 382), Loss: [3.20, 25.82]:  10%|         | 2/20 [00:05<00:45,  2.52s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:33.350601 4814732736 def_function.py:474] 5 out of the last 12 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->295=7.22 - Octave: 1, Shape: (573, 573), Loss: [4.28, 21.29]:  10%|         | 2/20 [00:07<01:10,  3.91s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:17:43.615088 4814732736 def_function.py:474] 5 out of the last 13 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->165=6.48->295=7.22 - Octave: 2, Shape: (860, 860), Loss: [3.10, 20.28]:  15%|        | 3/20 [00:18<01:44,  6.14s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->259=0.12 - Octave: 0, Shape: (382, 382), Loss: [0.01, 0.26]: 100%|| 20/20 [00:17<00:00,  1.17it/s]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->259=0.12 - Octave: 1, Shape: (573, 573), Loss: [0.01, 0.23]: 100%|| 20/20 [00:43<00:00,  2.17s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->259=0.12 - Octave: 2, Shape: (860, 860), Loss: [0.01, 0.19]: 100%|| 20/20 [01:43<00:00,  5.18s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->287=3.6 - Octave: 0, Shape: (382, 382), Loss: [0.08, 2.35]: 100%|| 20/20 [00:35<00:00,  1.78s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->287=3.6 - Octave: 1, Shape: (573, 573), Loss: [0.09, 2.15]: 100%|| 20/20 [00:50<00:00,  2.52s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->287=3.6 - Octave: 2, Shape: (860, 860), Loss: [0.07, 2.16]: 100%|| 20/20 [01:36<00:00,  4.84s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->265=7.3 - Octave: 0, Shape: (382, 382), Loss: [12.92, 25.60]:   5%|         | 1/20 [00:05<01:36,  5.08s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->265=7.3 - Octave: 1, Shape: (573, 573), Loss: [10.01, 20.01]:   5%|         | 1/20 [00:07<02:19,  7.32s/it]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->265=7.3 - Octave: 2, Shape: (860, 860), Loss: [8.54, 23.41]:  10%|         | 2/20 [00:16<02:25,  8.07s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:24:13.058007 4814732736 def_function.py:474] 5 out of the last 28 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->27=1.72 - Octave: 0, Shape: (382, 382), Loss: [6.60, 20.56]:  70%|   | 14/20 [00:05<00:02,  2.34it/s]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->27=1.72 - Octave: 1, Shape: (573, 573), Loss: [8.09, 22.77]:  35%|      | 7/20 [00:04<00:09,  1.41it/s]\n",
      "Images/Personal-Propic/o->117=4.24->127=4.15->27=1.72 - Octave: 2, Shape: (860, 860), Loss: [10.11, 20.69]:  15%|        | 3/20 [00:06<00:34,  2.01s/it]\n",
      "Images/Personal-Propic/o->290=1.5->88=1.79 - Octave: 0, Shape: (382, 382), Loss: [2.12, 9.80]: 100%|| 20/20 [00:10<00:00,  1.96it/s]\n",
      "Images/Personal-Propic/o->290=1.5->88=1.79 - Octave: 1, Shape: (573, 573), Loss: [2.61, 10.60]: 100%|| 20/20 [00:26<00:00,  1.32s/it]\n",
      "Images/Personal-Propic/o->290=1.5->88=1.79 - Octave: 2, Shape: (860, 860), Loss: [2.69, 19.12]: 100%|| 20/20 [00:58<00:00,  2.92s/it]\n",
      "Images/Personal-Propic/o->290=1.5->33=1.71 - Octave: 0, Shape: (382, 382), Loss: [3.40, 22.87]:  75%|  | 15/20 [00:08<00:02,  1.69it/s]\n",
      "Images/Personal-Propic/o->290=1.5->33=1.71 - Octave: 1, Shape: (573, 573), Loss: [8.68, 31.18]:  10%|         | 2/20 [00:03<00:29,  1.64s/it]\n",
      "Images/Personal-Propic/o->290=1.5->33=1.71 - Octave: 2, Shape: (860, 860), Loss: [25.73, 25.73]:   0%|          | 0/20 [00:02<?, ?it/s]\n",
      "Images/Personal-Propic/o->290=1.5->133=4.63 - Octave: 0, Shape: (382, 382), Loss: [3.74, 17.44]: 100%|| 20/20 [00:18<00:00,  1.11it/s]\n",
      "Images/Personal-Propic/o->290=1.5->133=4.63 - Octave: 1, Shape: (573, 573), Loss: [5.09, 26.60]:  35%|      | 7/20 [00:34<01:03,  4.87s/it]\n",
      "Images/Personal-Propic/o->290=1.5->133=4.63 - Octave: 2, Shape: (860, 860), Loss: [8.95, 28.55]:  10%|         | 2/20 [00:14<02:09,  7.22s/it]\n",
      "Images/Personal-Propic/o->290=1.5->232=8.75 - Octave: 0, Shape: (382, 382), Loss: [2.99, 20.52]:  40%|      | 8/20 [00:11<00:17,  1.44s/it]\n",
      "Images/Personal-Propic/o->290=1.5->232=8.75 - Octave: 1, Shape: (573, 573), Loss: [3.93, 20.54]:  45%|     | 9/20 [00:22<00:27,  2.52s/it]\n",
      "Images/Personal-Propic/o->290=1.5->232=8.75 - Octave: 2, Shape: (860, 860), Loss: [2.95, 20.63]:  55%|    | 11/20 [00:47<00:38,  4.28s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->254=0.32 - Octave: 0, Shape: (382, 382), Loss: [0.19, 11.79]: 100%|| 20/20 [00:18<00:00,  1.11it/s]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->254=0.32 - Octave: 1, Shape: (573, 573), Loss: [0.29, 15.28]: 100%|| 20/20 [00:38<00:00,  1.93s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->254=0.32 - Octave: 2, Shape: (860, 860), Loss: [0.25, 22.95]:  80%|  | 16/20 [01:09<00:17,  4.33s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->80=1.65 - Octave: 0, Shape: (382, 382), Loss: [2.90, 16.17]: 100%|| 20/20 [00:10<00:00,  1.99it/s]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->80=1.65 - Octave: 1, Shape: (573, 573), Loss: [3.36, 18.38]: 100%|| 20/20 [00:19<00:00,  1.01it/s]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->80=1.65 - Octave: 2, Shape: (860, 860), Loss: [3.21, 19.70]: 100%|| 20/20 [00:43<00:00,  2.17s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->136=6.21 - Octave: 0, Shape: (382, 382), Loss: [4.23, 21.43]:  40%|      | 8/20 [00:06<00:09,  1.28it/s]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->136=6.21 - Octave: 1, Shape: (573, 573), Loss: [4.79, 23.10]:  30%|       | 6/20 [00:09<00:22,  1.61s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->136=6.21 - Octave: 2, Shape: (860, 860), Loss: [7.64, 40.84]:  10%|         | 2/20 [00:11<01:43,  5.77s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->189=5.77 - Octave: 0, Shape: (382, 382), Loss: [9.01, 25.58]:  10%|         | 2/20 [00:03<00:32,  1.82s/it]\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->189=5.77 - Octave: 1, Shape: (573, 573), Loss: [9.42, 25.25]:  10%|         | 2/20 [00:05<00:53,  2.95s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:32:51.435099 4814732736 def_function.py:474] 5 out of the last 17 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->47=5.45->87=4.78->189=5.77 - Octave: 2, Shape: (860, 860), Loss: [9.06, 22.73]:  10%|         | 2/20 [00:11<01:46,  5.90s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0603 19:33:02.763184 4814732736 def_function.py:474] 5 out of the last 13 calls to <function gradient_ascent_step at 0x7fc4a35a0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4->292=8.64 - Octave: 0, Shape: (382, 382), Loss: [0.20, 9.26]: 100%|| 20/20 [00:20<00:00,  1.04s/it]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4->292=8.64 - Octave: 1, Shape: (573, 573), Loss: [0.28, 5.32]: 100%|| 20/20 [00:39<00:00,  1.98s/it]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4->292=8.64 - Octave: 2, Shape: (860, 860), Loss: [0.21, 6.67]: 100%|| 20/20 [01:27<00:00,  4.37s/it]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4->176=7.51 - Octave: 0, Shape: (382, 382), Loss: [1.67, 20.62]:  60%|    | 12/20 [00:10<00:07,  1.12it/s]\n",
      "Images/Personal-Propic/o->192=6.46->265=1.4->176=7.51 - Octave: 1, Shape: (573, 573), Loss: [1.95, 12.48]:  35%|      | 7/20 [00:13<00:25,  1.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b007f4c082d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mcoeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0msingle_layer_dreamify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f5270951b1ef>\u001b[0m in \u001b[0;36msingle_layer_dreamify\u001b[0;34m(source_filepath, layer_index, coeff, max_loss, display_image)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdestination_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_filepath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"->\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdreamify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-674087cb0f10>\u001b[0m in \u001b[0;36mdreamify\u001b[0;34m(source_filepath, destination_filepath, model, layer_settings, learning_rate, num_octave, octave_scale, iterations, max_loss)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                    \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                    \u001b[0mdestination_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdestination_filepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                    max_loss=max_loss)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Restores lost details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mupscaled_shrunk_original_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshrunk_original_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-48891d2d3182>\u001b[0m in \u001b[0;36mgradient_ascent_loop\u001b[0;34m(img, feature_extractor, layer_settings, iterations, learning_rate, octave, shape, destination_filepath, max_loss)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_ascent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"Images/Personal-Propic/\"\n",
    "iterations = 3\n",
    "n_layers = 4\n",
    "max_coeff = 10\n",
    "max_loss = 20\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    paths = [p for p in (glob(path + \"*.png\") + glob(path + \"*.jpg\"))]\n",
    "    \n",
    "    for source_path in paths:\n",
    "        \n",
    "        layers = np.random.randint(0, len(model.layers), n_layers)\n",
    "\n",
    "        for l in layers:\n",
    "            \n",
    "            coeff = np.random.uniform(high=max_coeff)\n",
    "            single_layer_dreamify(source_path, l, coeff, max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 [datascience]",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
